{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lineage Parser\n",
    "\n",
    "Extracts data lineage edges from Fabric Data Warehouse DDL definitions.\n",
    "\n",
    "**Pipeline:** `Source DWH (DDL) → Copy Activity → raw.* → This Notebook → meta.* → GraphQL → Frontend`\n",
    "\n",
    "**Sections:**\n",
    "1. Configuration - Database settings and URL patterns\n",
    "2. Extraction Rules - Regex patterns for DDL parsing\n",
    "3. Parser Class - Core extraction logic\n",
    "4. Test - Local validation without database\n",
    "5. Execute - Pipeline entry point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Database connection settings and URL classification patterns.\n",
    "These values rarely change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\"db_name\": \"db_datalineage\"}\n",
    "\n",
    "# Reference type enumeration\n",
    "REF_LOCAL    = 0  # schema.table (2-part name)\n",
    "REF_FILE     = 1  # Azure storage: abfss://, *.blob.core.windows.net\n",
    "REF_OTHER_DB = 2  # Cross-warehouse: database.schema.table (3-part)\n",
    "REF_LINK     = 3  # OneLake shortcut: *.dfs.fabric.microsoft.com\n",
    "\n",
    "# URL classification patterns\n",
    "STORAGE_PATTERNS  = ('abfss://', 'wasbs://', 'wasb://', 'adl://')\n",
    "FILE_URL_PATTERNS = ('.blob.core.windows.net', '.dfs.core.windows.net')\n",
    "LINK_URL_PATTERNS = ('.dfs.fabric.microsoft.com', '.blob.fabric.microsoft.com')\n",
    "\n",
    "# Comma-join pattern template (SQL-89 style)\n",
    "_COMMA_PATTERN = r'\\bFROM\\s+(?:[^\\s,;()]+(?:\\s+(?:AS\\s+)?\\w+)?\\s*,\\s*){%d}[^\\s,;()]+(?:\\s+(?:AS\\s+)?\\w+)?\\s*,\\s*([^\\s,;()]+)'\n",
    "_COMMA_JOIN_LIMIT = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extraction Rules\n",
    "\n",
    "Regex patterns that define what SQL constructs to extract.\n",
    "**Edit this section to add new patterns.**\n",
    "\n",
    "Each rule has:\n",
    "- `name` - Unique identifier for debugging\n",
    "- `target` - One of: `source`, `target`, `sp_call`, `external`\n",
    "- `pattern` - Regex with capture group `([...])` for table/path name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRACTION_RULES = [\n",
    "    # Source patterns (data flows FROM these objects)\n",
    "    {\"name\": \"from_join\",   \"target\": \"source\",   \"pattern\": r'\\b(?:FROM|JOIN|INNER\\s+JOIN|LEFT\\s+(?:OUTER\\s+)?JOIN|RIGHT\\s+(?:OUTER\\s+)?JOIN|FULL\\s+(?:OUTER\\s+)?JOIN|CROSS\\s+JOIN|OUTER\\s+JOIN)\\s+([^\\s,;()]+)'},\n",
    "    {\"name\": \"apply\",       \"target\": \"source\",   \"pattern\": r'\\b(?:CROSS\\s+APPLY|OUTER\\s+APPLY)\\s+([^\\s,;()]+)'},\n",
    "    {\"name\": \"merge_using\", \"target\": \"source\",   \"pattern\": r'\\bMERGE\\s+(?:INTO\\s+)?[^\\s,;()]+(?:\\s+(?:AS\\s+)?\\w+)?\\s+USING\\s+([^\\s,;()]+)'},\n",
    "    {\"name\": \"set_ops\",     \"target\": \"source\",   \"pattern\": r'\\b(?:EXCEPT|INTERSECT)\\s+(?:ALL\\s+)?SELECT\\s+.*?\\bFROM\\s+([^\\s,;()]+)'},\n",
    "] + [\n",
    "    # Comma-join patterns: FROM t1, t2, t3 -> captures t2, t3, etc.\n",
    "    {\"name\": f\"comma_{i+2}\", \"target\": \"source\", \"pattern\": _COMMA_PATTERN % i}\n",
    "    for i in range(_COMMA_JOIN_LIMIT - 1)\n",
    "] + [\n",
    "    # Target patterns (data flows INTO these objects)\n",
    "    {\"name\": \"dml\",         \"target\": \"target\",   \"pattern\": r'\\b(?:INSERT\\s+(?:INTO\\s+)?|UPDATE\\s+|MERGE\\s+(?:INTO\\s+)?|DELETE\\s+(?:FROM\\s+)?)([^\\s,;()]+)'},\n",
    "    {\"name\": \"ctas\",        \"target\": \"target\",   \"pattern\": r'\\bCREATE\\s+TABLE\\s+([^\\s,;()]+)\\s+AS\\s+SELECT'},\n",
    "    {\"name\": \"select_into\", \"target\": \"target\",   \"pattern\": r'\\bSELECT\\s+.*?\\s+INTO\\s+([^\\s,;()]+)\\s+FROM'},\n",
    "    {\"name\": \"copy_into\",   \"target\": \"target\",   \"pattern\": r'\\bCOPY\\s+INTO\\s+([^\\s,;()]+)'},\n",
    "    {\"name\": \"bulk_insert\", \"target\": \"target\",   \"pattern\": r'\\bBULK\\s+INSERT\\s+([^\\s,;()]+)'},\n",
    "\n",
    "    # Stored procedure calls\n",
    "    {\"name\": \"exec\",        \"target\": \"sp_call\",  \"pattern\": r'\\bEXEC(?:UTE)?\\s+([^\\s,;()]+)'},\n",
    "\n",
    "    # External file paths (in quotes)\n",
    "    {\"name\": \"openrowset\",  \"target\": \"external\", \"pattern\": r\"\\bOPENROWSET\\s*\\(\\s*BULK\\s+['\\\"]([^'\\\"]+)['\\\"]\"},\n",
    "    {\"name\": \"copy_from\",   \"target\": \"external\", \"pattern\": r\"\\bCOPY\\s+INTO\\s+\\S+\\s+FROM\\s+['\\\"]([^'\\\"]+)['\\\"]\"},\n",
    "    {\"name\": \"bulk_from\",   \"target\": \"external\", \"pattern\": r\"\\bBULK\\s+INSERT\\s+\\S+\\s+FROM\\s+['\\\"]([^'\\\"]+)['\\\"]\"},\n",
    "]\n",
    "\n",
    "# Comment hints for dynamic SQL\n",
    "HINT_PATTERNS = {\n",
    "    \"input\":  r'--\\s*@LINEAGE_INPUTS:\\s*(.+?)(?:\\n|$)',\n",
    "    \"output\": r'--\\s*@LINEAGE_OUTPUTS:\\s*(.+?)(?:\\n|$)',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parser Class\n",
    "\n",
    "Core extraction logic that processes DDL definitions and builds lineage edges.\n",
    "Do not modify unless changing parsing behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime, timezone\n",
    "from typing import Optional, Any\n",
    "from notebookutils import data\n",
    "from notebookutils import notebook as nb_utils\n",
    "\n",
    "\n",
    "class LineageParserError(Exception):\n",
    "    \"\"\"Raised when parser encounters a fatal error that should fail the pipeline.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class LineageParser:\n",
    "    \"\"\"Extracts data lineage edges from SQL DDL definitions.\n",
    "\n",
    "    Attributes:\n",
    "        cursor: Database cursor for executing queries.\n",
    "        catalog: Mapping of (source_id, name) to object_id.\n",
    "        debug: Whether to log detailed parsing steps.\n",
    "\n",
    "    Example:\n",
    "        >>> parser = LineageParser(connection, debug=True)\n",
    "        >>> print(parser.run())\n",
    "        'OK: 42 internal, 3 external edges (15 SPs, 8 views/functions)'\n",
    "    \"\"\"\n",
    "\n",
    "    _compiled: Optional[dict] = None\n",
    "\n",
    "    def __init__(self, connection: Optional[Any] = None, debug: bool = False) -> None:\n",
    "        \"\"\"Initialize the parser.\n",
    "\n",
    "        Args:\n",
    "            connection: Database connection from data.connect_to_artifact().\n",
    "            debug: If True, logs parsing steps to meta.parser_logs.\n",
    "        \"\"\"\n",
    "        self.cursor = connection.cursor() if connection else None\n",
    "        self.catalog: dict[tuple[int, str], int] = {}\n",
    "        self.sp_names: set[tuple[int, str]] = set()\n",
    "        self.debug = debug\n",
    "        self._debug_logs: list[dict] = []\n",
    "        self._run_id = str(uuid.uuid4()) if debug else None\n",
    "        self._run_start: Optional[datetime] = None\n",
    "        self._external_objects: list[tuple] = []\n",
    "\n",
    "    @classmethod\n",
    "    def _compile(cls) -> dict:\n",
    "        \"\"\"Compile and cache regex patterns.\"\"\"\n",
    "        if cls._compiled is None:\n",
    "            cls._compiled = {\n",
    "                \"rules\": [(r[\"target\"], re.compile(r[\"pattern\"], re.I | re.M | re.S)) for r in EXTRACTION_RULES],\n",
    "                \"hints\": {k: re.compile(v, re.I) for k, v in HINT_PATTERNS.items()},\n",
    "            }\n",
    "        return cls._compiled\n",
    "\n",
    "    @staticmethod\n",
    "    def _clean(ddl: str) -> str:\n",
    "        \"\"\"Remove SQL comments including nested block comments.\"\"\"\n",
    "        if not ddl:\n",
    "            return \"\"\n",
    "        ddl = re.sub(r'--[^\\r\\n]*', '', ddl)\n",
    "        result, i, depth = [], 0, 0\n",
    "        while i < len(ddl):\n",
    "            if ddl[i:i+2] == '/*':\n",
    "                depth += 1\n",
    "                i += 2\n",
    "            elif ddl[i:i+2] == '*/' and depth > 0:\n",
    "                depth -= 1\n",
    "                i += 2\n",
    "            elif depth == 0:\n",
    "                result.append(ddl[i])\n",
    "                i += 1\n",
    "            else:\n",
    "                i += 1\n",
    "        return ''.join(result)\n",
    "\n",
    "    @staticmethod\n",
    "    def _norm(name: str) -> str:\n",
    "        \"\"\"Normalize: remove brackets, lowercase, strip.\"\"\"\n",
    "        return name.replace('[', '').replace(']', '').lower().strip() if name else \"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _context(ddl: str, start: int, end: int, width: int = 50) -> str:\n",
    "        \"\"\"Extract DDL snippet for debug logging.\"\"\"\n",
    "        s, e = max(0, start - width), min(len(ddl), end + width)\n",
    "        return (\"...\" if s > 0 else \"\") + ddl[s:e].replace(\"\\n\", \" \") + (\"...\" if e < len(ddl) else \"\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _classify_reference(name: str) -> Optional[tuple[int, str]]:\n",
    "        \"\"\"Classify reference: (ref_type, name) or None if ignored.\"\"\"\n",
    "        name = name.replace('[', '').replace(']', '').strip()\n",
    "        name_lower = name.lower()\n",
    "\n",
    "        if name_lower.startswith(('#', '@')) or 'tempdb.' in name_lower:\n",
    "            return None\n",
    "        if any(name_lower.startswith(p) for p in STORAGE_PATTERNS):\n",
    "            return (REF_FILE, name)\n",
    "        if name_lower.startswith(('https://', 'http://')):\n",
    "            if any(p in name_lower for p in LINK_URL_PATTERNS):\n",
    "                return (REF_LINK, name)\n",
    "            if any(p in name_lower for p in FILE_URL_PATTERNS):\n",
    "                return (REF_FILE, name)\n",
    "            return None\n",
    "\n",
    "        parts = name.split('.')\n",
    "        if len(parts) == 2:\n",
    "            return (REF_LOCAL, name_lower)\n",
    "        if len(parts) == 3:\n",
    "            return (REF_OTHER_DB, name_lower)\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_display_name(ref_name: str, ref_type: int) -> str:\n",
    "        \"\"\"Extract short display name for UI.\"\"\"\n",
    "        if ref_type in (REF_FILE, REF_LINK):\n",
    "            return ref_name.replace('\\\\', '/').rstrip('/').split('/')[-1][:100]\n",
    "        if ref_type == REF_OTHER_DB:\n",
    "            parts = ref_name.split('.')\n",
    "            return '.'.join(parts[-2:])[:100] if len(parts) >= 2 else ref_name[:100]\n",
    "        return ref_name[:100]\n",
    "\n",
    "    def _check_raw_data(self) -> int:\n",
    "        \"\"\"Check if raw.objects has data. Returns count.\"\"\"\n",
    "        self.cursor.execute(\"SELECT COUNT(*) FROM raw.objects\")\n",
    "        return self.cursor.fetchone()[0]\n",
    "\n",
    "    def _register_sources(self) -> None:\n",
    "        \"\"\"Register new warehouse sources from raw.objects.\"\"\"\n",
    "        self.cursor.execute(\"\"\"\n",
    "            INSERT INTO meta.sources (server_name, database_name)\n",
    "            SELECT DISTINCT server_name, database_name FROM raw.objects o\n",
    "            WHERE NOT EXISTS (\n",
    "                SELECT 1 FROM meta.sources s\n",
    "                WHERE s.server_name = o.server_name AND s.database_name = o.database_name\n",
    "            )\n",
    "        \"\"\")\n",
    "        self.cursor.execute(\"EXEC meta.sp_set_active_source\")\n",
    "        self.cursor.connection.commit()\n",
    "\n",
    "    def _load_catalog(self) -> None:\n",
    "        \"\"\"Load object catalog for reference validation.\"\"\"\n",
    "        self.cursor.execute(\"EXEC meta.sp_get_catalog\")\n",
    "        for source_id, obj_id, name, obj_type in self.cursor.fetchall():\n",
    "            key = (source_id, name.lower())\n",
    "            self.catalog[key] = obj_id\n",
    "            if obj_type == 'Stored Procedure':\n",
    "                self.sp_names.add(key)\n",
    "\n",
    "    def _extract(self, ddl: str, target_type: str, source_id: int, steps: Optional[list] = None) -> dict:\n",
    "        \"\"\"Extract references from DDL using EXTRACTION_RULES.\"\"\"\n",
    "        local_refs: set[str] = set()\n",
    "        external_refs: list[tuple[int, str]] = []\n",
    "\n",
    "        for order, (target, regex) in enumerate(self._compile()[\"rules\"], 1):\n",
    "            if target != target_type:\n",
    "                continue\n",
    "\n",
    "            inputs, outputs, contexts = [], [], []\n",
    "            for m in regex.finditer(ddl):\n",
    "                match = m.group(1) if m.lastindex else m.group(0)\n",
    "                if isinstance(match, tuple):\n",
    "                    match = next((x for x in match if x), None)\n",
    "                if not match:\n",
    "                    continue\n",
    "\n",
    "                classified = self._classify_reference(match)\n",
    "                if not classified:\n",
    "                    continue\n",
    "\n",
    "                ref_type, name = classified\n",
    "                if ref_type == REF_LOCAL:\n",
    "                    if (source_id, name) in self.catalog:\n",
    "                        local_refs.add(name)\n",
    "                        (inputs if target == \"source\" else outputs).append(name)\n",
    "                        if steps is not None:\n",
    "                            contexts.append(self._context(ddl, m.start(), m.end()))\n",
    "                elif ref_type == REF_OTHER_DB:\n",
    "                    external_refs.append((ref_type, name))\n",
    "                    (inputs if target == \"source\" else outputs).append(f\"[OTHER_DB] {name}\")\n",
    "                elif ref_type in (REF_FILE, REF_LINK):\n",
    "                    external_refs.append((ref_type, name))\n",
    "                    outputs.append(f\"[{'FILE' if ref_type == REF_FILE else 'LINK'}] {name[:50]}\")\n",
    "\n",
    "            if steps is not None:\n",
    "                steps.append({\n",
    "                    \"order\": order, \"rule_name\": f\"{target_type}_{order}\", \"rule_target\": target,\n",
    "                    \"pattern\": regex.pattern[:150],\n",
    "                    \"inputs_found\": list(set(inputs)), \"outputs_found\": list(set(outputs)), \"match_context\": contexts,\n",
    "                })\n",
    "\n",
    "        return {\"local\": local_refs, \"external\": external_refs}\n",
    "\n",
    "    def _extract_hints(self, ddl: str, source_id: int) -> tuple[set, set]:\n",
    "        \"\"\"Extract @LINEAGE_INPUTS/@OUTPUTS comment hints.\"\"\"\n",
    "        inputs, outputs = set(), set()\n",
    "        hints = self._compile()[\"hints\"]\n",
    "        for match in hints[\"input\"].findall(ddl):\n",
    "            for item in match.split(','):\n",
    "                name = self._norm(item)\n",
    "                if (source_id, name) in self.catalog:\n",
    "                    inputs.add(name)\n",
    "        for match in hints[\"output\"].findall(ddl):\n",
    "            for item in match.split(','):\n",
    "                name = self._norm(item)\n",
    "                if (source_id, name) in self.catalog:\n",
    "                    outputs.add(name)\n",
    "        return inputs, outputs\n",
    "\n",
    "    def _parse_sp(self, source_id: int, obj_id: int, full_name: str, ddl: str) -> list[dict]:\n",
    "        \"\"\"Parse stored procedure DDL and extract edges.\"\"\"\n",
    "        edges: list[dict] = []\n",
    "        error_msg, status = None, \"success\"\n",
    "        raw_ddl = ddl\n",
    "        hints_in, hints_out = self._extract_hints(ddl, source_id)\n",
    "        cleaned = self._clean(ddl)\n",
    "        name = full_name.lower()\n",
    "        steps = [] if self.debug else None\n",
    "\n",
    "        try:\n",
    "            src = self._extract(cleaned, \"source\", source_id, steps)\n",
    "            tgt = self._extract(cleaned, \"target\", source_id, steps)\n",
    "            sp = self._extract(cleaned, \"sp_call\", source_id, steps)\n",
    "            ext = self._extract(cleaned, \"external\", source_id, steps)\n",
    "\n",
    "            sources, targets = src[\"local\"], tgt[\"local\"]\n",
    "            sp_calls = sp[\"local\"]\n",
    "            external_sources = src[\"external\"] + ext[\"external\"]\n",
    "            external_targets = tgt[\"external\"]\n",
    "\n",
    "            # UPDATE/DELETE alias fix: UPDATE o ... FROM dbo.Orders o\n",
    "            alias_pattern = r'\\b(?:UPDATE|DELETE)\\s+(\\w+)\\s+.*?\\bFROM\\s+([^\\s,;()]+)\\s+(?:AS\\s+)?\\1\\b'\n",
    "            for m in re.finditer(alias_pattern, cleaned, re.I | re.S):\n",
    "                real_table = self._norm(m.group(2))\n",
    "                sources.discard(real_table)\n",
    "                targets.add(real_table)\n",
    "\n",
    "            if steps is not None:\n",
    "                steps.append({\"order\": len(steps) + 1, \"rule_name\": \"hints\", \"rule_target\": \"hints\",\n",
    "                    \"pattern\": \"@LINEAGE_INPUTS / @LINEAGE_OUTPUTS\",\n",
    "                    \"inputs_found\": list(hints_in), \"outputs_found\": list(hints_out), \"match_context\": []})\n",
    "\n",
    "            for s in sources | hints_in:\n",
    "                if s != name and (sid := self.catalog.get((source_id, s))):\n",
    "                    edges.append({\"i\": source_id, \"s\": sid, \"t\": obj_id, \"st\": 0, \"tt\": 0})\n",
    "            for t in targets | hints_out:\n",
    "                if t != name and (tid := self.catalog.get((source_id, t))):\n",
    "                    edges.append({\"i\": source_id, \"s\": obj_id, \"t\": tid, \"st\": 0, \"tt\": 0})\n",
    "            for s in sp_calls:\n",
    "                if s != name and (source_id, s) in self.sp_names:\n",
    "                    if spid := self.catalog.get((source_id, s)):\n",
    "                        edges.append({\"i\": source_id, \"s\": obj_id, \"t\": spid, \"st\": 0, \"tt\": 0})\n",
    "\n",
    "            for ref_type, ref_name in external_sources:\n",
    "                self._external_objects.append((source_id, ref_type, ref_name, self._make_display_name(ref_name, ref_type)))\n",
    "                edges.append({\"i\": source_id, \"ext_src\": ref_name, \"t\": obj_id, \"st\": ref_type, \"tt\": 0})\n",
    "            for ref_type, ref_name in external_targets:\n",
    "                self._external_objects.append((source_id, ref_type, ref_name, self._make_display_name(ref_name, ref_type)))\n",
    "                edges.append({\"i\": source_id, \"s\": obj_id, \"ext_tgt\": ref_name, \"st\": 0, \"tt\": ref_type})\n",
    "\n",
    "        except Exception as e:\n",
    "            status, error_msg = \"error\", str(e)\n",
    "\n",
    "        if self.debug:\n",
    "            self._debug_logs.append({\"run_id\": self._run_id, \"run_started_at\": self._run_start.isoformat(),\n",
    "                \"db_name\": CONFIG[\"db_name\"], \"server_name\": str(source_id), \"sp_object_id\": obj_id,\n",
    "                \"sp_full_name\": full_name, \"raw_ddl\": raw_ddl, \"cleaned_ddl\": cleaned,\n",
    "                \"parsing_steps\": steps, \"edge_count\": len(edges), \"status\": status, \"error_message\": error_msg})\n",
    "        return edges\n",
    "\n",
    "    def _parse_view(self, source_id: int, obj_id: int, full_name: str, ddl: str) -> list[dict]:\n",
    "        \"\"\"Parse view/function DDL for external references.\"\"\"\n",
    "        edges: list[dict] = []\n",
    "        cleaned = self._clean(ddl)\n",
    "\n",
    "        try:\n",
    "            ext = self._extract(cleaned, \"external\", source_id)\n",
    "            src = self._extract(cleaned, \"source\", source_id)\n",
    "\n",
    "            for ref_type, ref_name in ext[\"external\"]:\n",
    "                if ref_type in (REF_FILE, REF_LINK):\n",
    "                    self._external_objects.append((source_id, ref_type, ref_name, self._make_display_name(ref_name, ref_type)))\n",
    "                    edges.append({\"i\": source_id, \"ext_src\": ref_name, \"t\": obj_id, \"st\": ref_type, \"tt\": 0})\n",
    "            for ref_type, ref_name in src[\"external\"]:\n",
    "                if ref_type == REF_OTHER_DB:\n",
    "                    self._external_objects.append((source_id, ref_type, ref_name, self._make_display_name(ref_name, ref_type)))\n",
    "                    edges.append({\"i\": source_id, \"ext_src\": ref_name, \"t\": obj_id, \"st\": ref_type, \"tt\": 0})\n",
    "\n",
    "        except Exception as e:\n",
    "            if self.debug:\n",
    "                self._debug_logs.append({\"run_id\": self._run_id, \"run_started_at\": self._run_start.isoformat(),\n",
    "                    \"db_name\": CONFIG[\"db_name\"], \"server_name\": str(source_id), \"sp_object_id\": obj_id,\n",
    "                    \"sp_full_name\": full_name, \"raw_ddl\": ddl, \"cleaned_ddl\": cleaned,\n",
    "                    \"parsing_steps\": [], \"edge_count\": 0, \"status\": \"error\", \"error_message\": str(e)})\n",
    "        return edges\n",
    "\n",
    "    def run(self) -> str:\n",
    "        \"\"\"Parse all objects and save lineage edges.\n",
    "        \n",
    "        Raises:\n",
    "            LineageParserError: If no data found in raw.objects (pipeline should fail).\n",
    "        \"\"\"\n",
    "        if self.debug:\n",
    "            self._run_start = datetime.now(timezone.utc)\n",
    "\n",
    "        self.cursor.execute(\"EXEC meta.sp_clear_parser_cache\")\n",
    "        self.cursor.connection.commit()\n",
    "\n",
    "        # Check raw.objects BEFORE registering sources\n",
    "        raw_count = self._check_raw_data()\n",
    "        if raw_count == 0:\n",
    "            error_msg = (\n",
    "                \"ERROR: No data in raw.objects table. \"\n",
    "                \"The Copy Pipeline must run first to copy DDL from source warehouse. \"\n",
    "                \"Check: 1) Copy Pipeline ran successfully, 2) Source DWH has objects, \"\n",
    "                \"3) Copy activity target is raw.objects table.\"\n",
    "            )\n",
    "            print(error_msg)\n",
    "            nb_utils.exit(error_msg)\n",
    "\n",
    "        self._register_sources()\n",
    "        self._load_catalog()\n",
    "\n",
    "        if not self.catalog:\n",
    "            error_msg = (\n",
    "                f\"ERROR: raw.objects has {raw_count} rows but no catalog entries. \"\n",
    "                \"This may indicate: 1) No source marked as active in meta.sources, \"\n",
    "                \"2) sp_get_catalog returned empty, 3) Object types not recognized.\"\n",
    "            )\n",
    "            print(error_msg)\n",
    "            nb_utils.exit(error_msg)\n",
    "\n",
    "        edges: list[dict] = []\n",
    "\n",
    "        self.cursor.execute(\"EXEC meta.sp_get_sp_definitions\")\n",
    "        sp_count = 0\n",
    "        for row in self.cursor.fetchall():\n",
    "            edges.extend(self._parse_sp(*row))\n",
    "            sp_count += 1\n",
    "\n",
    "        self.cursor.execute(\"EXEC meta.sp_get_view_definitions\")\n",
    "        vf_count = 0\n",
    "        for row in self.cursor.fetchall():\n",
    "            edges.extend(self._parse_view(*row))\n",
    "            vf_count += 1\n",
    "\n",
    "        if self._external_objects:\n",
    "            unique_ext = list({(i, r): (i, t, r, d) for i, t, r, d in self._external_objects}.values())\n",
    "            self.cursor.execute(\"EXEC meta.sp_save_external_objects @objects_json = ?\",\n",
    "                (json.dumps([{\"i\": i, \"t\": t, \"r\": r, \"d\": d} for i, t, r, d in unique_ext]),))\n",
    "\n",
    "        unique_edges = list({(e[\"i\"], e.get(\"s\"), e.get(\"t\"), e.get(\"ext_src\"), e.get(\"ext_tgt\")): e for e in edges}.values())\n",
    "        self.cursor.execute(\"EXEC meta.sp_save_parsed_edges @edges_json = ?\", (json.dumps(unique_edges),))\n",
    "        self.cursor.execute(\"EXEC meta.sp_compute_lineage\")\n",
    "\n",
    "        if self.debug and self._debug_logs:\n",
    "            self.cursor.execute(\"EXEC meta.sp_save_parser_log @logs_json = ?\", (json.dumps(self._debug_logs),))\n",
    "\n",
    "        self.cursor.connection.commit()\n",
    "\n",
    "        internal = sum(1 for e in unique_edges if \"s\" in e and \"ext_tgt\" not in e and \"ext_src\" not in e)\n",
    "        external = sum(1 for e in unique_edges if \"ext_src\" in e or \"ext_tgt\" in e)\n",
    "        result = f\"OK: {internal} internal, {external} external edges ({sp_count} SPs, {vf_count} views/functions)\"\n",
    "        if self._external_objects:\n",
    "            result += f\", {len(set((i,r) for i,_,r,_ in self._external_objects))} ext refs\"\n",
    "        if self.debug:\n",
    "            result += f\" (logged {len(self._debug_logs)} objects)\"\n",
    "        return result\n",
    "\n",
    "    @classmethod\n",
    "    def test(cls, ddl_text: str, debug: bool = False) -> str:\n",
    "        \"\"\"Test extraction without database connection.\"\"\"\n",
    "        parser = cls(debug=debug)\n",
    "        parser.catalog = {}\n",
    "        parser._run_start = datetime.now(timezone.utc)\n",
    "        source_id = 1\n",
    "\n",
    "        for rule in EXTRACTION_RULES:\n",
    "            for m in re.finditer(rule[\"pattern\"], ddl_text, re.I | re.M | re.S):\n",
    "                match = m.group(1) if m.lastindex else m.group(0)\n",
    "                if isinstance(match, tuple):\n",
    "                    match = next((x for x in match if x), None)\n",
    "                if match:\n",
    "                    classified = cls._classify_reference(match)\n",
    "                    if classified and classified[0] == REF_LOCAL:\n",
    "                        parser.catalog[(source_id, classified[1])] = hash(classified[1])\n",
    "\n",
    "        cleaned = cls._clean(ddl_text)\n",
    "        steps = [] if debug else None\n",
    "\n",
    "        src = parser._extract(cleaned, \"source\", source_id, steps)\n",
    "        tgt = parser._extract(cleaned, \"target\", source_id, steps)\n",
    "        sp = parser._extract(cleaned, \"sp_call\", source_id, steps)\n",
    "        ext = parser._extract(cleaned, \"external\", source_id, steps)\n",
    "\n",
    "        sources, targets = src[\"local\"], tgt[\"local\"]\n",
    "        external = src[\"external\"] + tgt[\"external\"] + ext[\"external\"]\n",
    "\n",
    "        alias_pattern = r'\\b(?:UPDATE|DELETE)\\s+(\\w+)\\s+.*?\\bFROM\\s+([^\\s,;()]+)\\s+(?:AS\\s+)?\\1\\b'\n",
    "        for m in re.finditer(alias_pattern, cleaned, re.I | re.S):\n",
    "            real_table = cls._norm(m.group(2))\n",
    "            sources.discard(real_table)\n",
    "            targets.add(real_table)\n",
    "\n",
    "        lines = [f\"Sources: {sorted(sources)}\", f\"Targets: {sorted(targets)}\", f\"SP Calls: {sorted(sp['local'])}\"]\n",
    "        if external:\n",
    "            lines.append(f\"External: {[('FILE' if t == REF_FILE else 'LINK' if t == REF_LINK else 'OTHER_DB', n) for t, n in external]}\")\n",
    "        if debug:\n",
    "            lines.extend([\"\", f\"Cleaned:\\n{cleaned}\", \"\", f\"Steps:\\n{json.dumps(steps, indent=2)}\"])\n",
    "        return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test\n",
    "\n",
    "Validate parser locally before deployment.\n",
    "Uses `LineageParser.test()` which requires no database connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(LineageParser.test(\"\"\"\n",
    "CREATE PROCEDURE dbo.LoadOrders AS\n",
    "BEGIN\n",
    "    SELECT * FROM dbo.Orders o\n",
    "    JOIN dbo.Customers c ON o.customer_id = c.id;\n",
    "\n",
    "    INSERT INTO dbo.OrderSummary\n",
    "    SELECT * FROM dbo.Orders;\n",
    "\n",
    "    EXEC dbo.NotifyComplete;\n",
    "END\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time\n\ndef connect_with_retry(db_name: str, max_attempts: int = 5) -> Any:\n    \"\"\"Connect to Fabric SQL Database with retry for cold start.\n    \n    Args:\n        db_name: SQL Database display name (case-sensitive)\n        max_attempts: Maximum connection attempts\n        \n    Returns:\n        Database connection object\n        \n    Raises:\n        Exception: If all connection attempts fail\n    \"\"\"\n    for attempt in range(1, max_attempts + 1):\n        wait_secs = 0 if attempt == 1 else min(30 * attempt, 120)\n        \n        if attempt > 1:\n            print(f\"  Waiting {wait_secs}s (cold start can take 2-3 min)...\")\n            time.sleep(wait_secs)\n        \n        try:\n            print(f\"  Attempt {attempt}/{max_attempts}...\", end=\" \")\n            conn = data.connect_to_artifact(db_name)\n            print(\"Connected!\")\n            return conn\n        except Exception as e:\n            err = str(e)\n            print(\"Timeout\" if \"timeout\" in err.lower() else f\"Failed: {err[:80]}\")\n            \n            if attempt == max_attempts:\n                raise Exception(f\"\"\"\nConnection failed after {max_attempts} attempts.\n\nTroubleshooting:\n- Database name '{db_name}' must match EXACTLY (case-sensitive)\n- Notebook must be in SAME workspace as SQL Database  \n- Try opening database in Fabric Portal first to wake it up\n- Check your permissions to the SQL Database\n\nError: {err[:200]}\n\"\"\")"
  },
  {
   "cell_type": "code",
   "source": "# Connect and run parser\nprint(f\"Connecting to: {CONFIG['db_name']}\")\nconn = connect_with_retry(CONFIG[\"db_name\"])\n\nparser = LineageParser(conn, debug=True)\nprint(parser.run())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "jupyter",
   "jupyter_kernel_name": "python3.11"
  },
  "kernelspec": {
   "name": "jupyter",
   "display_name": "Jupyter"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "save_output": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}